# An example of config file for the parameters used to train
# neural networks to predict the next state based on the current state and input signal 

# The file that contains the training and testing dataset
train_data_file: datatrain.pkl

# The file used to save the results of the training -> No extension, it will be autoset by the algorithm
output_file: result

# An identifier to uniquely name the parameter of this NN
model_name: test

# Optimizer and its parameters
optimizer:
  name: adam
  learning_rate_init: 0.002   # The learning rate at the first iteration of the gradient descent
  learning_rate_end: 0.001    # The learning rate at the last iteration of the gradient descent
  weight_decay: 0.0001        # Weight decay coefficient
  grad_clip: 0.01             # Gradient clip coefficient


# Regularization penalty for constraints on the midpoint (if enable)
pen_constr:
  pen_ineq_init: 0.001              # Initial penalty coefficient for the inequality constraints
  beta_ineq: 1.5                    # Multiplier coefficient used to update the penalty coefficient
  tol_constraint_ineq: 0.001        # Threshold to attest theat the inequality constraints are satisified
  # Each update of the SGD takes a batch of points to evaluate the satisfaction of constraints
  batch_size_test: 16               # Number of data taken in the test set
  batch_size_coloc: 16              # Number of data taken in the colocation set (if given)

# The seed for randomness and reproducibility
seed: [101, 201, 501]

# Batch size 
batch_size: 64

# Total number of iterations
num_gradient_iterations: 10000

# Frequency at which to compute loss function on the training and testing
freq_accuracy: [1.0, 100, 100]

# An integer that specifies if applying early stopping or not.
# Using early stopping criteria, patience specifies the number of step (in term of freq_accuracy) before deciding if we have the best solution or not
patience: -1

# Frequence of printing information and saving in the file
freq_save: 200

# Specify if the loss function is divided by the number of states (in addition to the batch size)
normalize: False

# The parameterization of the unknown terms of the vector field
# To be commented if the dynamics is fully known and we are just learning the remainder
# nn_params:
#   vector_field:
#     output: 2                         # Specify the number of outputs of this function
#     input_index: [0,1]                # Specify the input indexes of this function in the joint state control vector = [state, control]
#     output_sizes: [256, 256]          # Specify the size of the hidden layers only
#     activation: tanh                  # Activation function
#     b_init:
#       initializer: Constant           # Initializer of the biais value
#       params:
#         constant: 0                   # arguments of Constant initlaizer
#     w_init:
#       initializer: RandomUniform      # Initializer of the weight values randomly between the boubds below
#       params:
#         minval: -0.1
#         maxval: 0.1
#   # If another component is unknwon add it as follows with names that will be used in the vector field definition
#   vector_field_2:
#     output_sizes: [256, 256]          # Specify the size of the hidden layers only
#     activation: tanh                  # Activation function
#     b_init:
#       initializer: Constant           # Initializer of the biais value
#       params:
#         constant: 0                   # arguments of Constant initlaizer
#     w_init:
#       initializer: RandomUniform      # Initializer of the weight values randomly between the boubds below
#       params:
#         minval: -0.1
#         maxval: 0.1


# The parameterization of the integration method to use
baseline_params:
  name: tayla       # Use our taylor lagrange expansion with learned remainder/midpoint: name = tayla (ours), taylor(truncated taylor expansion), rk4, odeint(adtaptive step rk4)
  order: 1          # Specify the total order of the taylor expansion (the remainder term is given by order) --> this field is only valid for taylor and taylor method names
  # This attribute should be either midpoint or remainder if the method name is tayla
  # Otherwise it can't be ignored if the method name is rk4, taylor, odeint, or euler.
  # No need to specify output or input_index --> They are automatically infered from the number of states and control inputs of the system
  midpoint: # Or remainder
    output_sizes: [16, 16]            # Specify the size of the hidden layers only
    activation: tanh                  # Activation function
    b_init:
      initializer: Constant           # Initializer of the biais value
      params:
        constant: 0                   # arguments of Constant initlaizer
    w_init:
      initializer: RandomUniform      # Initializer of the weight values randomly between the boubds below
      params:
        minval: -0.1
        maxval: 0.1